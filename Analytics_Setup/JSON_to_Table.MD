# JSON to Table Pipeline

## Summary
This script establishes a data pipeline for processing JSON data in Snowflake. It creates the necessary schema, file formats, tables, and automation components to handle both initial and ongoing JSON data ingestion. The pipeline includes a scheduled task to continuously load new JSON files and a stored procedure to orchestrate the entire process with controlled timing and state management.

## Step-by-Step Description

### 1. Schema Setup
```sql
CREATE SCHEMA IF NOT EXISTS MED_DEVICE_TRANSCRIPTS.ANALYTICS;
```
Creates the ANALYTICS schema in the MED_DEVICE_TRANSCRIPTS database if it doesn't already exist. This schema will contain all objects related to the JSON processing pipeline.

### 2. File Format Definition
```sql
CREATE OR REPLACE FILE FORMAT MED_DEVICE_TRANSCRIPTS.ANALYTICS.JSON_GZ_FORMAT
	TYPE=JSON
    STRIP_OUTER_ARRAY=TRUE
    REPLACE_INVALID_CHARACTERS=TRUE
    DATE_FORMAT=AUTO
    TIME_FORMAT=AUTO
    TIMESTAMP_FORMAT=AUTO;
```
Defines a file format specification for handling JSON files. This format:
- Specifies that the files are in JSON format
- Automatically removes the outer array in the JSON structure
- Replaces any invalid characters
- Auto-detects date, time, and timestamp formats

### 3. Initial Data Table
```sql
CREATE OR REPLACE TABLE MED_DEVICE_TRANSCRIPTS.ANALYTICS.RAW_JSON_DATA_INITIAL (
    FILE_NAME VARCHAR,
    FILE_LOAD_TIME TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),
    JSON_DATA VARIANT
);
```
Creates a table to store the initial JSON data load. The table has columns for:
- File name
- Load timestamp (automatically set to the current time)
- The JSON content stored as a VARIANT type, which can hold semi-structured data

### 4. Load Initial Data
```sql
COPY INTO "MED_DEVICE_TRANSCRIPTS"."ANALYTICS"."RAW_JSON_DATA_INITIAL" 
FROM (SELECT 
    METADATA$FILENAME,
    CURRENT_TIMESTAMP(),
    $1::VARIANT
    FROM '@"MED_DEVICE_TRANSCRIPTS"."DATA_PREP"."CALL_DATA_INITIAL"') 
FILE_FORMAT = '"MED_DEVICE_TRANSCRIPTS"."ANALYTICS"."JSON_GZ_FORMAT"' 
ON_ERROR=ABORT_STATEMENT;
```
Copies the initial JSON files from the CALL_DATA_INITIAL stage into the RAW_JSON_DATA_INITIAL table. For each file, it captures:
- The file name (using METADATA$FILENAME)
- The current timestamp
- The file contents as VARIANT data

### 5. New Data Table
```sql
CREATE OR REPLACE TABLE MED_DEVICE_TRANSCRIPTS.ANALYTICS.RAW_JSON_DATA_NEW (
    FILE_NAME VARCHAR,
    FILE_LOAD_TIME TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP(),
    JSON_DATA VARIANT
);
```
Creates a table to store new JSON data that's generated by the ongoing data flow. The structure is the same as the initial data table.

### 6. Scheduled Task Creation
```sql
CREATE OR REPLACE TASK MED_DEVICE_TRANSCRIPTS.ANALYTICS.LOAD_JSON_FILES_NEW
    WAREHOUSE = CORTEX_DEMO_WH
    SCHEDULE = '15 seconds'
AS
    COPY INTO "MED_DEVICE_TRANSCRIPTS"."ANALYTICS"."RAW_JSON_DATA_NEW" 
    FROM (SELECT 
        METADATA$FILENAME,
        CURRENT_TIMESTAMP(),
        $1::VARIANT
        FROM '@"MED_DEVICE_TRANSCRIPTS"."DATA_PREP"."CALL_DATA_NEW"') 
    FILE_FORMAT = '"MED_DEVICE_TRANSCRIPTS"."ANALYTICS"."JSON_GZ_FORMAT"' 
    ON_ERROR=ABORT_STATEMENT;
```
Defines a scheduled task that runs every 15 seconds to:
- Copy any new JSON files from the CALL_DATA_NEW stage into the RAW_JSON_DATA_NEW table
- Use the CORTEX_DEMO_WH warehouse for compute resources
- Similar to the initial load, captures file name, timestamp, and file contents

### 7. Task Control Commands
```sql
-- Resume the task to start processing
ALTER TASK MED_DEVICE_TRANSCRIPTS.ANALYTICS.LOAD_JSON_FILES_NEW RESUME;

-- Suspend the task
ALTER TASK MED_DEVICE_TRANSCRIPTS.ANALYTICS.LOAD_JSON_FILES_NEW SUSPEND;
```
These commands allow for manual control of the task:
- RESUME starts the scheduled task so it begins running every 15 seconds
- SUSPEND stops the scheduled task from running

### 8. Batch Processing Command
```sql
-- Call the new procedure to process conversations (default 3 times)
CALL MED_DEVICE_TRANSCRIPTS.DATA_PREP.PROCESS_CONVERSATIONS_BATCH();
```
Calls a procedure (defined in the DATA_PREP schema) that processes conversation data in batches.

### 9. Orchestration Procedure
```sql
CREATE OR REPLACE PROCEDURE MED_DEVICE_TRANSCRIPTS.ANALYTICS.RUN_NEW_TRANSCRIPT_PIPELINE()
    RETURNS STRING
    LANGUAGE JAVASCRIPT
    EXECUTE AS CALLER
AS
$$
    try {
        // Resume the task
        var resume_stmt = snowflake.createStatement({
            sqlText: "ALTER TASK MED_DEVICE_TRANSCRIPTS.ANALYTICS.LOAD_JSON_FILES_NEW RESUME;"
        });
        resume_stmt.execute();
        
        // Call the batch processing with 3 iterations
        var batch_stmt = snowflake.createStatement({
            sqlText: "CALL MED_DEVICE_TRANSCRIPTS.DATA_PREP.PROCESS_CONVERSATIONS_BATCH(3);"
        });
        batch_stmt.execute();
        
        // Add a 16 second delay using Snowflake's system$wait function so that the last file created is loaded into the table
        var wait_stmt = snowflake.createStatement({
            sqlText: "CALL system$wait(16);"
        });
        wait_stmt.execute();
        
        // Suspend the task after processing completes and delay
        var suspend_stmt = snowflake.createStatement({
            sqlText: "ALTER TASK MED_DEVICE_TRANSCRIPTS.ANALYTICS.LOAD_JSON_FILES_NEW SUSPEND;"
        });
        suspend_stmt.execute();
        
        return "Batch processing completed successfully: Task resumed, 3 batches processed, waited 16 seconds, then task suspended.";
    } catch (err) {
        return "Error during batch processing: " + err;
    }
$$;
```
This JavaScript stored procedure orchestrates the full pipeline:
1. Resumes the LOAD_JSON_FILES_NEW task to start loading new files
2. Calls the PROCESS_CONVERSATIONS_BATCH procedure with 3 iterations
3. Waits 16 seconds to ensure the last generated file is loaded into the table
4. Suspends the task once processing is complete
5. Returns a success message or error details if something fails

### 10. Procedure Execution
```sql
CALL MED_DEVICE_TRANSCRIPTS.ANALYTICS.RUN_NEW_TRANSCRIPT_PIPELINE();
```
Executes the orchestration procedure to run the entire pipeline. 